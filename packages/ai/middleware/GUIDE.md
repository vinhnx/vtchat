# Language Model Middleware Guide (AI SDK v4)\n\nLanguage model middleware is a way to enhance the behavior of language models by intercepting and modifying the calls to the language model.\n\nIt can be used to add features like guardrails, RAG, caching, and logging in a language model agnostic way. Such middleware can be developed and distributed independently from the language models that they are applied to.\n\n## Using Language Model Middleware\n\nYou can use language model middleware with the `wrapLanguageModel` function. It takes a language model and a language model middleware and returns a new language model that incorporates the middleware.\n\n```ts\nimport { wrapLanguageModel } from 'ai';\n\nconst wrappedLanguageModel = wrapLanguageModel({\n  model: yourModel,\n  middleware: yourLanguageModelMiddleware,\n});\n```\n\nThe wrapped language model can be used just like any other language model, e.g. in `streamText`:\n\n```ts highlight=\"2\"\nconst result = streamText({\n  model: wrappedLanguageModel,\n  prompt: 'What cities are in the United States?',\n});\n```\n\n## Multiple Middlewares\n\nYou can provide multiple middlewares to the `wrapLanguageModel` function. The middlewares will be applied in the order they are provided.\n\n```ts\nconst wrappedLanguageModel = wrapLanguageModel({\n  model: yourModel,\n  middleware: [firstMiddleware, secondMiddleware],\n});\n\n// applied as: firstMiddleware(secondMiddleware(yourModel))\n```\n\n## Built-in Middleware\n\nThe AI SDK comes with several built-in middlewares that you can use to configure language models:\n\n- `extractReasoningMiddleware`: Extracts reasoning information from the generated text and exposes it as a `reasoning` property on the result.\n- `simulateStreamingMiddleware`: Simulates streaming behavior with responses from non-streaming language models.\n- `defaultSettingsMiddleware`: Applies default settings to a language model.\n\n### Extract Reasoning\n\nSome providers and models expose reasoning information in the generated text using special tags, e.g. `\\` and `\\`.\n\nThe `extractReasoningMiddleware` function can be used to extract this reasoning information and expose it as a `reasoning` property on the result.\n\n```ts\nimport { wrapLanguageModel, extractReasoningMiddleware } from 'ai';\n\nconst model = wrapLanguageModel({\n  model: yourModel,\n  middleware: extractReasoningMiddleware({ tagName: 'think' }),\n});\n```\n\nYou can then use that enhanced model in functions like `generateText` and `streamText`.\n\nThe `extractReasoningMiddleware` function also includes a `startWithReasoning` option. When set to `true`, the reasoning tag will be prepended to the generated text. This is useful for models that do not include the reasoning tag at the beginning of the response. For more details, see the [DeepSeek R1 guide](/docs/guides/r1#deepseek-r1-middleware).\n\n### Simulate Streaming\n\nThe `simulateStreamingMiddleware` function can be used to simulate streaming behavior with responses from non-streaming language models. This is useful when you want to maintain a consistent streaming interface even when using models that only provide complete responses.\n\n```ts\nimport { wrapLanguageModel, simulateStreamingMiddleware } from 'ai';\n\nconst model = wrapLanguageModel({\n  model: yourModel,\n  middleware: simulateStreamingMiddleware(),\n});\n```\n\n### Default Settings\n\nThe `defaultSettingsMiddleware` function can be used to apply default settings to a language model.\n\n```ts\nimport { wrapLanguageModel, defaultSettingsMiddleware } from 'ai';\n\nconst model = wrapLanguageModel({\n  model: yourModel,\n  middleware: defaultSettingsMiddleware({\n    settings: {\n      temperature: 0.5,\n      maxTokens: 800,\n      // note: use providerMetadata instead of providerOptions here:\n      providerMetadata: { openai: { store: false } },\n    },\n  }),\n});\n```\n\n## Implementing Language Model Middleware\n\n<Note>\n  Implementing language model middleware is advanced functionality and requires\n  a solid understanding of the [language model\n  specification](https://github.com/vercel/ai/blob/main/packages/provider/src/language-model/v1/language-model-v1.ts).\n</Note>\n\nIn AI SDK v4, you implement middleware using the `LanguageModelV2Middleware` interface, which has been updated from the previous version. You can implement any of the following functions to modify the behavior of the language model:\n\n1. `transformParams`: Transforms the parameters before they are passed to the language model, for both `doGenerate` and `doStream`.\n2. `wrapGenerate`: Wraps the `doGenerate` method of the [language model](https://github.com/vercel/ai/blob/main/packages/provider/src/language-model/v1/language-model-v1.ts). You can modify the parameters, call the language model, and modify the result.\n3. `wrapStream`: Wraps the `doStream` method of the [language model](https://github.com/vercel/ai/blob/main/packages/provider/src/language-model/v1/language-model-v1.ts). You can modify the parameters, call the language model, and modify the result.\n\nHere are some examples of how to implement language model middleware:\n\n## Examples\n\n<Note>\n  These examples are not meant to be used in production. They are just to show\n  how you can use middleware to enhance the behavior of language models.\n</Note>\n\n### Logging\n\nThis example shows how to log the parameters and generated text of a language model call.\n\n```ts\nimport type { LanguageModelV2Middleware, LanguageModelV2StreamPart } from '@ai-sdk/provider';\n\nexport const yourLogMiddleware: LanguageModelV2Middleware = {\n  wrapGenerate: async ({ doGenerate, params }) => {\n    console.log('doGenerate called');\n    console.log(`params: ${JSON.stringify(params, null, 2)}`);\n\n    const result = await doGenerate();\n\n    console.log('doGenerate finished');\n    console.log(`generated text: ${result.text}`);\n\n    return result;\n  },\n\n  wrapStream: async ({ doStream, params }) => {\n    console.log('doStream called');\n    console.log(`params: ${JSON.stringify(params, null, 2)}`);\n\n    const { stream, ...rest } = await doStream();\n\n    let generatedText = '';\n\n    const transformStream = new TransformStream<\n      LanguageModelV2StreamPart,\n      LanguageModelV2StreamPart\n    >({\n      transform(chunk, controller) {\n        if (chunk.type === 'text-delta') {\n          generatedText += chunk.textDelta;\n        }\n\n        controller.enqueue(chunk);\n      },\n\n      flush() {\n        console.log('doStream finished');\n        console.log(`generated text: ${generatedText}`);\n      },\n    });\n\n    return {\n      stream: stream.pipeThrough(transformStream),\n      ...rest,\n    };\n  },\n};\n```\n\n### Caching\n\nThis example shows how to build a simple cache for the generated text of a language model call.\n\n```ts\nimport type { LanguageModelV2Middleware } from '@ai-sdk/provider';\n\nconst cache = new Map<string, any>();\n\nexport const yourCacheMiddleware: LanguageModelV2Middleware = {\n  wrapGenerate: async ({ doGenerate, params }) => {\n    const cacheKey = JSON.stringify(params);\n\n    if (cache.has(cacheKey)) {\n      return cache.get(cacheKey);\n    }\n\n    const result = await doGenerate();\n\n    cache.set(cacheKey, result);\n\n    return result;\n  },\n\n  // here you would implement the caching logic for streaming\n};\n```\n\n### Retrieval Augmented Generation (RAG)\n\nThis example shows how to use RAG as middleware.\n\n<Note>\n  Helper functions like `getLastUserMessageText` and `findSources` are not part\n  of the AI SDK. They are just used in this example to illustrate the concept of\n  RAG.\n</Note>\n\n```ts\nimport type { LanguageModelV2Middleware } from '@ai-sdk/provider';\n\nexport const yourRagMiddleware: LanguageModelV2Middleware = {\n  transformParams: async ({ params }) => {\n    const lastUserMessageText = getLastUserMessageText({\n      prompt: params.prompt,\n    });\n\n    if (lastUserMessageText == null) {\n      return params; // do not use RAG (send unmodified parameters)\n    }\n\n    const instruction =\n      'Use the following information to answer the question:\\n' +\n      findSources({ text: lastUserMessageText })\n        .map(chunk => JSON.stringify(chunk))\n        .join('\\n');\n\n    return addToLastUserMessage({ params, text: instruction });\n  },\n};\n```\n\n### Guardrails\n\nGuard rails are a way to ensure that the generated text of a language model call is safe and appropriate. This example shows how to use guardrails as middleware.\n\n```ts\nimport type { LanguageModelV2Middleware } from '@ai-sdk/provider';\n\nexport const yourGuardrailMiddleware: LanguageModelV2Middleware = {\n  wrapGenerate: async ({ doGenerate }) => {\n    const { text, ...rest } = await doGenerate();\n\n    // filtering approach, e.g. for PII or other sensitive information:\n    const cleanedText = text?.replace(/badword/g, '<REDACTED>');\n\n    return { text: cleanedText, ...rest };\n  },\n\n  // here you would implement the guardrail logic for streaming\n  // Note: streaming guardrails are difficult to implement, because\n  // you do not know the full content of the stream until it's finished.\n};\n```\n\n## Configuring Per Request Custom Metadata\n\nTo send and access custom metadata in Middleware, you can use `providerMetadata`. This is useful when building logging middleware where you want to pass additional context like user IDs, timestamps, or other contextual data that can help with tracking and debugging.\n\n```ts\nimport { openai } from '@ai-sdk/openai';\nimport { generateText, wrapLanguageModel, LanguageModelV2Middleware } from 'ai';\n\nexport const yourLogMiddleware: LanguageModelV2Middleware = {\n  wrapGenerate: async ({ doGenerate, params }) => {\n    console.log('METADATA', params?.providerMetadata?.yourLogMiddleware);\n    const result = await doGenerate();\n    return result;\n  },\n};\n\nconst { text } = await generateText({\n  model: wrapLanguageModel({\n    model: openai('gpt-4o'),\n    middleware: yourLogMiddleware,\n  }),\n  prompt: 'Invent a new holiday and describe its traditions.',\n  providerMetadata: {\n    yourLogMiddleware: {\n      hello: 'world',\n    },\n  },\n});\n\nconsole.log(text);\n```\n\n## VT Chat Middleware System\n\nVT Chat implements an enhanced middleware system that builds upon the AI SDK v4 capabilities with additional features:\n\n### Flexible Configuration System\n\nVT Chat provides a flexible configuration system that allows enabling/disabling middleware based on context:\n\n```ts\ninterface MiddlewareConfig {\n  enableLogging?: boolean;\n  enableCaching?: boolean;\n  enableGuardrails?: boolean;\n  customMiddleware?: LanguageModelV2Middleware[];\n}\n```\n\n### Presets for Common Use Cases\n\nVT Chat includes predefined middleware presets:\n\n- `DEVELOPMENT`: Enables logging and guardrails\n- `PRODUCTION`: Enables caching and guardrails\n- `PERFORMANCE`: Enables caching only\n- `PRIVACY`: Enables guardrails only (no caching to avoid storing sensitive data)\n\n### Combining Multiple Middleware Sources\n\nVT Chat's implementation allows combining directly provided middleware with configured middleware:\n\n```ts\n// Combine provided middleware with our configured middleware\nconst allMiddleware = [\n  ...(middleware ? (Array.isArray(middleware) ? middleware : [middleware]) : []),\n  ...getMiddlewareForContext(m, middlewareConfig),\n];\n```\n\nThis approach provides maximum flexibility for different usage scenarios while maintaining clean separation of concerns.